---
title: "Python Logistic Regression"
description: |
  Under construction. 
author:
  - name: Ryan Plain
    url: {}
date: 08-19-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(reticulate)

```


# Logistic Regression

## Why R and Python

> "R and Python are like my right and left hand. I can do everything in both, but one is clumsier than the other." -[Demitri](https://twitter.com/PhDemetri)

I was listening to the [Learn Bayes Stats](https://www.learnbayesstats.com/) podcast with Alex Andorra and guest Demitri, and heard this line. I loved it. You hear all the time about, "Which should I learn for Data Science, R or Python?" This is usually followed up by regurgitated answers: 

- Python if you have a computer science background, R if you come from statistics. 
- R users bashing on anything other than R (primarily because software engineers have created an ecosystem where you never have to leave this beautiful language)
- Python because it is so versatile (another favorite quote from the podcast is that Python is the 2nd best language at everything). 

*Occasionally* you'll hear, learn both. This should always be the answer in my opinion. 

I first learned Python, primarily because I was graduating with a Business Analytics degree that had us using drag and drop neural networks without learning anything more than a hidden layer. I quickly realized this didn't set me up for much of anything, and probably should have stuck with the less trendy Economics degree. 

Python was indeed more versatile, and at this point I just needed to land any job in the realms of IT. I will say that if I had to go back, I would learn Python again first and then R. This is just due to the fact that solidifying computer science concepts is a foundation of how I use R today. Since I primarily work with data, 80% of my time wrangling or cleaning, I use R. If I need to understand the math in better detail, it is easier for me to view Python tutorials from scratch due to the explicit classes and sub classes. I also find it subjectively *more elegant* than R. 

There are tasks that each language is better suited for, knowing both allows a synergy effect. Whether it is with your colleagues, being versatile in the work you can do, or with yourself. Having a deep understanding of each language in your toolbox allows you to leverage the strengths of both to become the best version of a data scientist you can be. 

**Most likely** it is because the underlying concepts transfer over to all problems, and deep down it is really just the syntax or interface to the solution. For all I know, [Julia](https://julialang.org/) could take over as the number 1 language for data science by the time I finish this series of posts. 

Maybe I should be using that already...

## The series

I'll be doing a series of modeling, in both languages, as I hope to grasp as much of the concepts as possible. Currently, I'm working through the second edition of [Introduction to Statistical Learning *Second Edition*](https://www.statlearning.com/) and plan to implement models as I read through it. 

I'm pretty sure I am speaking into the abyss here, but it is a way to keep me accountable!


## The Content

This is just a quick Logistic Regression model implemented in both languages. I tried to do a model with NFL data, like most of my blogs. However, it turned out to be quite tedious setting up the data, and I ended up scrapping it down to something I knew would pass. I'm fairly comfortable running standard linear regression as well as logistic, so this is just something to get up on the board and move into deeper topics. 

Ultimately, I would like to manufacture a dataset I can dive deeper on assumptions, or feature engineering. The **LAST** thing I want is to show exporting sklearn and running a Logistic Regression. As I've learned from so many people smarter than me, the dangers of ML come with applying algorithms without thinking about the data. How is it collected? What kind of interpretability does this analysis require? 

Anyone can import the library and run the model, Data Science is much more than that.

## The Data

Load in 2020 play by play data from **nflreadr**.

```{r}
pbp <- nflreadr::load_pbp(2019:2020, file_type = 'rds')
```

I did a basic summary of each game, the teams, winner, and expected points added stats. For a detailed description on EPA, you can find at [NFL.com](https://www.nfl.com/news/next-gen-stats-new-advanced-metrics-you-need-to-know-for-the-2020-nfl-season).

```{r}
library(tidyverse)
library(zoo)

pbp %>% 
  filter(week <= 16) %>% 
  mutate(
    home_off_epa = ifelse(posteam == home_team & play == 1, epa, NA),
    away_off_epa = ifelse(posteam == away_team & play == 1, epa, NA),
    home_def_epa = ifelse(defteam == home_team & play == 1, epa, NA),
    away_def_epa = ifelse(defteam == away_team & play == 1, epa, NA)
  ) %>% 
  group_by(home_team, away_team, game_date) %>%
  summarise(
    home_score = max(total_home_score, na.rm = T),
    away_score = max(total_away_score, na.rm = T),
    home_off_epa = mean(home_off_epa, na.rm = T),
    away_off_epa = mean(away_off_epa, na.rm = T),
    home_def_epa = mean(home_def_epa, na.rm = T),
    away_def_epa = mean(away_def_epa, na.rm = T),
    .groups = 'drop'
  ) %>% 
  mutate(
    game_date = lubridate::ymd(game_date)
  ) %>% 
  arrange(game_date) %>% 
  mutate(outcome = case_when(
    home_score > away_score ~ 'home',
    away_score > home_score ~ 'away',
    TRUE ~ 'Tie'
  )) %>% 
  filter(outcome != 'Tie') -> games

games %>% 
  mutate(outcome = outcome == 'home') %>% 
  rename_with(~ str_replace(., "home", "team")) %>% 
  rename_with(~ str_replace(., "away", "opp")) -> home_teams


games %>% 
  mutate(outcome = outcome == 'home') %>% 
  rename_with(~ str_replace(., "away", "team")) %>% 
  rename_with(~ str_replace(., "home", "opp")) %>% 
  mutate(outcome = outcome == FALSE) -> away_teams

```

## The R Model

To model the games, I used a rolling average of each teams last 4 games offensive and defensive mean EPA. To get the early weeks of 2020, I used 2019 games (excluding playoffs and week 17). 

This is a good foundation, however it could be improved by adding in coaching and personel data. Tom Brady switching from the New England to Tampa essentially makes the 2019 New England Patriots data irrelevant to 2020 success. 

Pro Football Focus has developed a [WAR](https://www.pff.com/war) metric for the NFL, finding that QB's have wins above replacement between 2 - 6 games where other positions are lucky to see outlier seasons eclipse 1 win above replacement. An improved model would take into account the off season changes and bake them in addition to the team level EPA metrics. 


```{r}
df <- bind_rows(home_teams, away_teams)

df <- df %>% 
  arrange(game_date) %>% 
  group_by(team_team) %>% 
  mutate(
    across(starts_with("team_"), ~rollmean(., 4, align = 'right', fill = NA))
    ) %>% 
  ungroup() %>% 
  group_by(opp_team) %>% 
  arrange(game_date) %>% 
  mutate(
    across(starts_with("opp_"), ~rollmean(., 4, align = 'right', fill = NA))
  ) %>% 
  filter(lubridate::year(game_date) == 2020) %>% 
  ungroup()

df %>% 
  select(contains('epa'), outcome) %>% 
  mutate(outcome = ifelse(outcome == TRUE, 1, 0)) -> glm_data


mylogit <- glm(outcome ~ ., data = glm_data, family = 'binomial')

glm_data$pred <-predict(mylogit, newdata = df %>% select(-outcome))

glm_data %>% 
  mutate(pred = exp(pred)/(1+exp(pred))) %>% 
  mutate(pred = ifelse(pred >= .5, 1, 0)) %>% 
  count(outcome == pred) %>% 
  mutate(acc = n / sum(n))

summary(mylogit)

data <- reticulate::r_to_py(glm_data)
```

## The Python Model

By default, sklearn already imposes a regularization penalty. I'll be diving more into that in a later post. 

Import the libraries used for Logistic Regression. 

```{python}
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix

from sklearn.model_selection import train_test_split, KFold, LeaveOneOut
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.datasets import load_diabetes

```

The same data, without the R predictions. 

```{python}

data = r.data.drop(['pred'], axis=1)
data['outcome'] = data['outcome'].astype(int)
print(data.head())
```


```{python}

predictors = ['team_off_epa', 'opp_off_epa', 'team_def_epa', 'opp_def_epa']

X_train, X_test, y_train, y_test = train_test_split(data[predictors], data['outcome'], test_size = 0.2)

zscore = StandardScaler()
zscore.fit(X_train)

Xz_train = zscore.transform(X_train)
Xz_test = zscore.transform(X_test)
```

I actually much prefer the API for sklearn. Python and the OOP framework is easier for me to understand how the logic is applied. R, Tidyverse, and Tidymodels are simple to use, but a lot of times it is difficult for me to debug complex problems because of how much of the technical implementation is abstracted away. 

That is also why it is the elite tool for EDA!

```{python}
myLogit = LogisticRegression()

myLogit.fit(Xz_train, y_train)

predictedVals = myLogit.predict(Xz_test)
```

```{python}
accuracy_score(y_test, predictedVals)

confusion_matrix(y_test, predictedVals)
```

### Cross Validation 

```{python}
X = data[predictors]
y = data['outcome']

kf = KFold(n_splits = 5)
lr = LogisticRegression()

acc = []
```

```{python}

for train_indicies, test_indicies in kf.split(X):
  
  X_train = X.iloc[train_indicies]
  X_test = X.iloc[test_indicies]
  y_train = y[train_indicies]
  y_test = y[test_indicies]
  
  z = StandardScaler()
  z.fit(X_train)
  
  Xz_train = zscore.transform(X_train)
  Xz_test = z.transform(X_test)
  
  model = lr.fit(Xz_train, y_train)
  acc.append(accuracy_score(y_test, model.predict(Xz_test)))
  
  
print(acc)
print(np.mean(acc))
```

```{python}
coef = pd.DataFrame({'Coefs': myLogit.coef_[0], 'Names':predictors})
coef = coef.append({'Coefs': myLogit.intercept_[0], 'Names':'intercept'}, ignore_index=True)
coef
```

### Odds

Odds show the exponentiated log odds provided from the logit model. Odds means that for every 1 standard deviation increase in X will increase by a factor of the odds. 

Odds are usually more intuitive for stakeholders. 

In this instance, an increase in opponents offensive EPA weighs more heavily than an an increase in the individual teams offensive EPA. 

```{python}

coef['Odds Coef'] = np.exp(coef['Coefs'])
coef

```

## Adjust the probability for classification of a win

Threshold modifications won't do anything to improve performance with this data. Depending on what you are classifying, you can adjust the risk tolerance by changing the threshold of the probabilities. 

For marketing, 20% might be enough to justify sending an ad. In healthcare, a 90% probability might be necessary.

For our data, it absolutely doesn't matter... I'll try to build a better NFL dataset and drop in here!

```{python}
X_new = data.iloc[:, 0:4].copy()
Xnewz = zscore.transform(X_new)

Ypred_prob = myLogit.predict_proba(Xnewz)
Ypred_prob[1:5]
```

```{python}

Ypred_prob1 = Ypred_prob[:,1]

thresh = 0.75

Ypred_prob1_thresh = (Ypred_prob1 > thresh) * 1

Ypred_prob1_thresh


accuracy_score(data['outcome'], Ypred_prob1_thresh)


```


That's it! Either a better logistic model next time, Tidymodels, or the next chapter in ISLR2.  









